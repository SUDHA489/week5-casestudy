{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0deb9b11-99e2-454b-819f-6af7ab693896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydeequ\n",
      "  Downloading pydeequ-1.5.0-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /databricks/python3/lib/python3.12/site-packages (from pydeequ) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from pydeequ) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=0.23.0->pydeequ) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=0.23.0->pydeequ) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23.0->pydeequ) (1.16.0)\n",
      "Downloading pydeequ-1.5.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pydeequ\n",
      "Successfully installed pydeequ-1.5.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3843fc2-1257-4c85-96a1-1a3f013e8ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd97d15-6d8b-41e7-b3f0-8afb819675c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd176b15-ed00-4d7c-9bd3-5f76376d3a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading processed IoT data...\n",
      "Loaded 20 records from processed IoT data\n",
      "Data Quality Check Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:163: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>check</th><th>check_level</th><th>check_status</th><th>constraint</th><th>constraint_status</th><th>constraint_message</th></tr></thead><tbody><tr><td>IoT Sensor Data Quality</td><td>Error</td><td>Success</td><td>ComplianceConstraint(Compliance(Status contained in OK,WARNING,ALERT,`Status` IS NULL OR `Status` IN ('OK','WARNING','ALERT'),None))</td><td>Success</td><td></td></tr><tr><td>IoT Sensor Data Quality</td><td>Error</td><td>Success</td><td>ComplianceConstraint(Compliance(Temperature_C is non-negative,COALESCE(CAST(Temperature_C AS DECIMAL(20,10)), 0.0) >= 0,None))</td><td>Success</td><td></td></tr><tr><td>IoT Sensor Data Quality</td><td>Error</td><td>Success</td><td>ComplianceConstraint(Compliance(Pressure_bar is non-negative,COALESCE(CAST(Pressure_bar AS DECIMAL(20,10)), 0.0) >= 0,None))</td><td>Success</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "IoT Sensor Data Quality",
         "Error",
         "Success",
         "ComplianceConstraint(Compliance(Status contained in OK,WARNING,ALERT,`Status` IS NULL OR `Status` IN ('OK','WARNING','ALERT'),None))",
         "Success",
         ""
        ],
        [
         "IoT Sensor Data Quality",
         "Error",
         "Success",
         "ComplianceConstraint(Compliance(Temperature_C is non-negative,COALESCE(CAST(Temperature_C AS DECIMAL(20,10)), 0.0) >= 0,None))",
         "Success",
         ""
        ],
        [
         "IoT Sensor Data Quality",
         "Error",
         "Success",
         "ComplianceConstraint(Compliance(Pressure_bar is non-negative,COALESCE(CAST(Pressure_bar AS DECIMAL(20,10)), 0.0) >= 0,None))",
         "Success",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "check",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "check_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "check_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "constraint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "constraint_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "constraint_message",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality report saved to abfss://iot-reports@iotdatastoragebalu.dfs.core.windows.net/deequ_report\n",
      "DQ Check complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading processed IoT data...\")\n",
    "\n",
    "storage_account = \"iotdatastoragebalu\"\n",
    "storage_key = \"\"\n",
    "container_processed = \"iot-processed\"\n",
    "container_reports = \"iot-reports\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    storage_key\n",
    ")\n",
    "\n",
    "df = spark.read.format(\"parquet\").load(\n",
    "    f\"abfss://{container_processed}@{storage_account}.dfs.core.windows.net/\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {df.count()} records from processed IoT data\")\n",
    "\n",
    "\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import json\n",
    "\n",
    "check = (\n",
    "    Check(spark, CheckLevel.Error, \"IoT Sensor Data Quality\")\n",
    "    .isContainedIn(\"Status\", [\"OK\", \"WARNING\", \"ALERT\"])\n",
    "    .isNonNegative(\"Temperature_C\")\n",
    "    .isNonNegative(\"Pressure_bar\")\n",
    ")\n",
    "\n",
    "result = (\n",
    "    VerificationSuite(spark)\n",
    "    .onData(df)\n",
    "    .addCheck(check)\n",
    "    .run()\n",
    ")\n",
    "\n",
    "print(\"Data Quality Check Results:\")\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "display(result_df)\n",
    "\n",
    "result_path = f\"abfss://{container_reports}@{storage_account}.dfs.core.windows.net/deequ_report\"\n",
    "result_df.write.mode(\"overwrite\").json(result_path)\n",
    "\n",
    "print(f\"Data Quality report saved to {result_path}\")\n",
    "print(\"DQ Check complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IoT_DQ_Checks.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
